# Elegant-Expression
Useful sentences collected from papers.

---

1. Understanding how and why certain neural networks perform better than others remains an **art**.

   > Visualizing the PHATE of Neural Networks, NIPS 2019

2. Global attention can only be used after significant spatial downsampling has been applied to the input because it is computationally expensive, which prevents its usage across all layers in a fully attentional model

   > Stand-Alone Self-Attention in Vision Models, NIPS 2019

3. particularly as an improvement on **simplistic** regularization such as norms.

   > Supervised autoencoders: Improving generalization performance with unsupervised regularizers, NIPS 2018

4. Adversarial examples have been investigated in the **seminal** work of Biggio et al. (2013).

   > Adversarial Interpolation Training: A Simple Approach for Improving Model Robustness, ICLR 2020 Under review

5. While traversals in the latent representation can qualitatively illustrate disentanglement, quantitative measures of disentanglement are in their **infancy**.

   > Isolating Sources of Disentanglement in Variational Autoencoders, NIPS 2018

6. Evaluating generative models is hard, there are no clear-cut success criteria for autoencoder reconstruction.

   > Adversarial Images for Variational Autoencoders, NIPS 2016 Workshop

7. - ... we have omitted the corresponding input vectors from the right-hand side of the conditioning statements to simplify the notation.
   - Here we have omitted the dependence on the input variable $\textbf{x}$ to keep the notation uncluttered.

   > Pattern Recognition and Machine Learning. Page 156 & Page 165.

8. In this work, we are able to answer both of these questions **affirmatively**, by proposing and analyzing ...

   > Communication-Efficient Asynchronous Stochastic Frank-Wolfe over Nuclear-norm Balls, arXiv preprint.

9. 