# Elegant-Expression
Useful sentences collected from papers.

---

1. Understanding how and why certain neural networks perform better than others remains an **art**.

   > Visualizing the PHATE of Neural Networks, NIPS 2019

2. Global attention can only be used after significant spatial downsampling has been applied to the input because it is computationally expensive, which prevents its usage across all layers in a fully attentional model

   > Stand-Alone Self-Attention in Vision Models, NIPS 2019

3. particularly as an improvement on **simplistic** regularization such as norms.

   > Supervised autoencoders: Improving generalization performance with unsupervised regularizers, NIPS 2018

4. Adversarial examples have been investigated in the **seminal** work of Biggio et al. (2013).

   > Adversarial Interpolation Training: A Simple Approach for Improving Model Robustness, ICLR 2020 Under review

5. While traversals in the latent representation can qualitatively illustrate disentanglement, quantitative measures of disentanglement are in their **infancy**.

   > Isolating Sources of Disentanglement in Variational Autoencoders, NIPS 2018

6. Evaluating generative models is hard, there are no clear-cut success criteria for autoencoder reconstruction.

   > Adversarial Images for Variational Autoencoders, NIPS 2016 Workshop

7. - ... we have omitted the corresponding input vectors from the right-hand side of the conditioning statements to simplify the notation.
   - Here we have omitted the dependence on the input variable $\textbf{x}$ to keep the notation uncluttered.

   > Pattern Recognition and Machine Learning. Page 156 & Page 165.

8. In this work, we are able to answer both of these questions **affirmatively**, by proposing and analyzing ...

   > Communication-Efficient Asynchronous Stochastic Frank-Wolfe over Nuclear-norm Balls, arXiv preprint, 2019.

9. The matrix dimensions $d$ and $m$ are both **in the thousands or above**, and the number of functions (data points) $n$ is **in the millions or more**.

   > A distributed Frankâ€“Wolfe framework for learning low-rank matrices with the trace norm, Machine Learning, 2018.

10. We present an optimization method that offers empirically **the best of both worlds**: our algorithm yields good generalization performance while requiring only one hyper-parameter.

    > Deep Frank-Wolfe For Neural Network Optimization, ICLR 2019.

11. The quality of each model can be appreciated in figure.

    > Adversarial Attacks on Variational Autoencoders, arXiv preprint, 2018

12. Pushing the **envelope** with a lager model.

    > Adversarial Examples Improve Image Recognition, arXiv preprint, 2019

13. But **even with this note of caution**, we believe this work offers **exciting avenues** for future work.

    > Are Generative Classifiers More Robust to Adversarial Attacks?, ICML 2019.

14. It is therefore not surprising that generative models have gained **momentum** such as computer vision, NLP and chemistry.

    > From Variational to Deterministic Autoencoders, ICLR 2020

15. Learning a VAE **amounts to** the optimization of an objective **balancing** the quality of samples that are autoencoded through a stochastic encoder-decoder pair **while** encouraging the latent space to follow a fixed prior distribution.

    > From Variational to Deterministic Autoencoders, ICLR 2020

16. Since their introduction, VAEs have become **one of the frameworks of choice** among the different generative models.

    > From Variational to Deterministic Autoencoders, ICLR 2020

17. A major weakness of VAEs is the tendency to **strike an unsatisfying compromise between** sample quality **and** reconstruction quality.

    > From Variational to Deterministic Autoencoders, ICLR 2020

18. We thank all the reviewers for **appreciating the direction of our work** and its relevance for the generative modeling community.  We agree that the evaluation of generative models is a nontrivial problem and a very active field. We **appreciate** the **constructive criticisms** and would **gladly accommodate additional experiments** that would improve the paper's quality.

    > https://openreview.net/forum?id=S1g7tpEYDS

19. **Striving** for simplicity, we employ ...

    > From Variational to Deterministic Autoencoders, ICLR 2020

20. While computing the Jacobian for CAEs is close **in spirit** to $\mathcal{L}_{\mathrm{GP}}$for RAEs.

    > From Variational to Deterministic Autoencoders, ICLR 2020

21. VQ-VAEs **necessitates** complex discrete autoregressive density estimators and a training loss that is non-differentiable due to quantizing $\mathrm{Z}$.

    > From Variational to Deterministic Autoencoders, ICLR 2020

22. The evaluation of generative models is a nontrivial question. We report here the ubiquitous FID ...

    > From Variational to Deterministic Autoencoders, ICLR 2020

23. While the theoretical derivation of the VAE has helped popularize the framework for generative modeling, recent works have started to expose some **discrepancies between theory and practice**.

    > From Variational to Deterministic Autoencoders, ICLR 2020

24. 
