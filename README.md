# Elegant-Expression
Useful sentences collected from papers.

---

1. Understanding how and why certain neural networks perform better than others remains an **art**.

   > Visualizing the PHATE of Neural Networks, NIPS 2019

2. Global attention can only be used after significant spatial downsampling has been applied to the input because it is computationally expensive, which prevents its usage across all layers in a fully attentional model

   > Stand-Alone Self-Attention in Vision Models, NIPS 2019

3. particularly as an improvement on **simplistic** regularization such as norms.

   > Supervised autoencoders: Improving generalization performance with unsupervised regularizers, NIPS 2018

4. Adversarial examples have been investigated in the **seminal** work of Biggio et al. (2013).

   > Adversarial Interpolation Training: A Simple Approach for Improving Model Robustness, ICLR 2020 Under review

5. While traversals in the latent representation can qualitatively illustrate disentanglement, quantitative measures of disentanglement are in their **infancy**.

   > Isolating Sources of Disentanglement in Variational Autoencoders, NIPS 2018

6. Evaluating generative models is hard, there are no clear-cut success criteria for autoencoder reconstruction.

   > Adversarial Images for Variational Autoencoders, NIPS 2016 Workshop

7. - ... we have omitted the corresponding input vectors from the right-hand side of the conditioning statements to simplify the notation.
   - Here we have omitted the dependence on the input variable $\textbf{x}$ to keep the notation uncluttered.

   > Pattern Recognition and Machine Learning. Page 156 & Page 165.

8. In this work, we are able to answer both of these questions **affirmatively**, by proposing and analyzing ...

   > Communication-Efficient Asynchronous Stochastic Frank-Wolfe over Nuclear-norm Balls, arXiv preprint, 2019.

9. The matrix dimensions $d$ and $m$ are both **in the thousands or above**, and the number of functions (data points) $n$ is **in the millions or more**.

   > A distributed Frank–Wolfe framework for learning low-rank matrices with the trace norm, Machine Learning, 2018.

10. We present an optimization method that offers empirically **the best of both worlds**: our algorithm yields good generalization performance while requiring only one hyper-parameter.

    > Deep Frank-Wolfe For Neural Network Optimization, ICLR 2019.

11. The quality of each model can be appreciated in figure.

    > Adversarial Attacks on Variational Autoencoders, arXiv preprint, 2018

12. Pushing the **envelope** with a lager model.

    > Adversarial Examples Improve Image Recognition, arXiv preprint, 2019

<<<<<<< HEAD
13. Developments in zeroth order optimization has been **fueled** by various applications ranging from problems in medical science, material science and chemistry.

    >Towards Gradient Free and Projection Free Stochastic Optimization, AISTATS 2019

14. 
=======
13. But **even with this note of caution**, we believe this work offers **exciting avenues** for future work.

    > Are Generative Classifiers More Robust to Adversarial Attacks?, ICML 2019.

14. It is therefore not surprising that generative models have gained **momentum** such as computer vision, NLP and chemistry.

    > From Variational to Deterministic Autoencoders, ICLR 2020

15. Learning a VAE **amounts to** the optimization of an objective **balancing** the quality of samples that are autoencoded through a stochastic encoder-decoder pair **while** encouraging the latent space to follow a fixed prior distribution.

    > From Variational to Deterministic Autoencoders, ICLR 2020

16. Since their introduction, VAEs have become **one of the frameworks of choice** among the different generative models.

    > From Variational to Deterministic Autoencoders, ICLR 2020

17. A major weakness of VAEs is the tendency to **strike an unsatisfying compromise between** sample quality **and** reconstruction quality.

    > From Variational to Deterministic Autoencoders, ICLR 2020

18. We thank all the reviewers for **appreciating the direction of our work** and its relevance for the generative modeling community.  We agree that the evaluation of generative models is a nontrivial problem and a very active field. We **appreciate** the **constructive criticisms** and would **gladly accommodate additional experiments** that would improve the paper's quality.

    > https://openreview.net/forum?id=S1g7tpEYDS

19. **Striving** for simplicity, we employ ...

    > From Variational to Deterministic Autoencoders, ICLR 2020

20. While computing the Jacobian for CAEs is close **in spirit** to $\mathcal{L}_{\mathrm{GP}}$for RAEs.

    > From Variational to Deterministic Autoencoders, ICLR 2020

21. VQ-VAEs **necessitates** complex discrete autoregressive density estimators and a training loss that is non-differentiable due to quantizing $\mathrm{Z}$.

    > From Variational to Deterministic Autoencoders, ICLR 2020

22. The evaluation of generative models is a nontrivial question. We report here the ubiquitous FID ...

    > From Variational to Deterministic Autoencoders, ICLR 2020

23. While the theoretical derivation of the VAE has helped popularize the framework for generative modeling, recent works have started to expose some **discrepancies between theory and practice**.

    > From Variational to Deterministic Autoencoders, ICLR 2020

24. **The bulk of research** in variational inference over the years has been **on ways in which to** compute the gradient of the expected log-likelihood $\nabla_{\phi} \mathbb{E}_{q_{\phi}(z)}[\log p(\mathbf{x} | \mathbf{z})]$.

    > Variational Inference with Normalizing Flows, ICML 2015

25. In this paper we work towards this **ultimate vision**, in addition to intermediate applications, by aiming to improve upon the state-of-the-art of generative models.

    > Glow: Generative flow with invertible 1×1 convolutions, NIPS 2018

26. The **discipline** of generative modeling **has experienced enormous leaps** in **capabilities** in recent years.

    > Glow: Generative flow with invertible 1×1 convolutions, NIPS 2018

27. GANs are **arguably** best known for their ability to synthesize large and realistic images.

    > Glow: Generative flow with invertible 1×1 convolutions, NIPS 2018

28. The **essence** of the framework is the class `Engine`.

    > https://pytorch.org/ignite/concepts.html

29. **Provided by courtesy of** the authors of [17].

    > Variational autoencoders pursue pca directions (by accident), CVPR 2019

30. Exemplarily, a latent traversal for a $\beta$-VAE is shown in Fig. 1

    > Variational autoencoders pursue pca directions (by accident), CVPR 2019

31. Such observations have **planted a suspicion** that the **inner workings** of the VAE are not sufficiently understood.

    > Variational autoencoders pursue pca directions (by accident), CVPR 2019

32. **as we are about to see**, this diagonalization comes with beneficial effects **regarding** disentanglement.

    > Variational autoencoders pursue pca directions (by accident), CVPR 2019

33. One of the key **aspirations** in recent machine learning research is to ...

    > BIVA: A Very Deep Hierarchy of Latent Variables for Generative Modeling, NIPS 2019

34. The **ever-increasing** size of modern data sets ...

    > Semi-supervised learning with deep generative models, NIPS 2014

35. Such problems are **of immense practical interest** in a wide range of applications.

    > Semi-supervised learning with deep generative models, NIPS 2014

36. These adversarial at- tacks present a security risk to deployed ANNs and indicate a **divergence between how ANNs and humans** perform classification.

    > Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks, ICLR 2020

37. Although artificial neural networks (ANNs) have recently begun to **rival** human performance on various tasks.

    > Biologically inspired sleep algorithm for increased generalization and adversarial robustness in deep neural networks, ICLR 2020

38. Current attack methods lack control over where the attack occurs, **opting to** attack all pixels **indiscriminately**.

    > Unrestricted Adversarial Examples via Semantic Manipulation, ICLR 2020

39. Though there could be **oscillations**, we find that the attack strengths **do grow monotonically with respect to** the evolution iterations in our experiments.

    > NATTACK: Learning the Distributions of Adversarial Examples for an Improved Black-Box Attack on Deep Neural Networks, ICML 2019

40. It evades a standard MNIST model **using** just **12 queries on average**. Similar performance is observed on a standard IMAGENET model **with** **an average of 579 queries**.

    > Sign Bits Are All You Need for Black-Box Attacks, ICLR 2020

41. **Central to** their approaches is estimating the gradient.

    > Sign Bits Are All You Need for Black-Box Attacks, ICLR 2020

42. After the **dissemination** of this work, a more general result was published in ...

    > Certified Adversarial Robustness via Randomized Smoothing, ICML 2019

43. Note that the stability objective used here is **similar in spirit to**, but **different in context from**, the stability training used in Zheng et al. (2016); Li et al. (2019).

    > Black-box Smoothing: A Provable Defense for Pretrained Classifiers, ICLR 2020 Workshop

44. we investigate this intriguing, yet **thus far** overlooked **aspect** of adversarial training.

    > Improving Adversarial Robustness Requires Revisiting Misclassified Examples, ICLR 2020

45. We encourage readers to pause after this section to **reflect on** how one might attack the described defense before reading our method.

    > On Adaptive Attacks to Adversarial Example Defenses, arxiv 2020

46. Importantly, we do not wish to claim that the robustness techniques used by many of these defenses hold **no merit whatsoever**.

    > On Adaptive Attacks to Adversarial Example Defenses, arxiv 2020

47. This **mirrors** our own **mental model** when assessing the robustness of this defense.

    > On Adaptive Attacks to Adversarial Example Defenses, arxiv 2020

48. Rather than **delving into** the theory of Variational Auto-Encoders, we present here the high-level classification algorithm implemented in the source code released by the authors.

    > On Adaptive Attacks to Adversarial Example Defenses, arxiv 2020

49. Deep generative models **tend to** be much more complex than standard discriminative models, so the complexity of this defense is not **unwarranted**.

    > On Adaptive Attacks to Adversarial Example Defenses, arxiv 2020

50. **Towards that end**, we perform the analysis that **we would have liked to see** in the original paper.

    > On Adaptive Attacks to Adversarial Example Defenses, arxiv 2020

51. While **theoretically grounded arguments** are encouraged, it is counterproductive to add **“decorative math”** whose primary purpose is to make the submission look more substantial or even intimidating, without adding significant insight.

    > https://nips.cc/Conferences/2020/CallForPapers
    
52. The attack themes we distill in Section 4 are not **hard-and-fast rules**: it is possible to evade defenses while also (**knowingly**) disregarding them.

    > On Adaptive Attacks to Adversarial Example Defenses, arxiv 2020

53. Our guess **as to why this may happen** comes from our experience with the original attack codebase, which is not a simple attack and can thus be hard to debug.

    > On Adaptive Attacks to Adversarial Example Defenses, arxiv 2020

54. This phenomenon has **been widely studied both from the theoretical and empirical perspectives** , and remains such a hallmark of deep learning practice that **it is often taken for granted.**

    > Overfitting in adversarially robust deep learning, arxiv 2020

55. The big **catch** is that we have to choose a way to quantify the size $\left\| \nabla \mathcal{D}\right\|$ of the discriminator’s gradients!

    > https://f-t-s.github.io/projects/icr/

56. Our adversarial all-layer margin **circumvents** this lower bound because it considers all layers of the network rather than just the output.

    > Improved Sample Complexities for Deep Networks and Robust Classification via an All-Layer Margin, ICLR 2020

57. which is more **amenable** for capturing complex distribution manifested in the data.

    > You Look Twice: GaterNet for Dynamic Filter Selection in CNNs, CVPR 2019

58. The core of the idea is to introduce a dedicated gater network to **take a glimpse of the input**, and then generate input-dependent binary gates to select filters in the backbone network for processing the input.

    > You Look Twice: GaterNet for Dynamic Filter Selection in CNNs, CVPR 2019

59. That will likely introduce new and healthy **accountability** into your research lives.

    > https://medium.com/@BrentH/suggestions-for-writing-neurips-2020-broader-impacts-statements-121da1b765bf

60. 

>>>>>>> 82f3cd79a263d99e3b8670849130a2db123baaaa
